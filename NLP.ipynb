{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d092877-4b32-4bf7-839a-55e8166a0d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b150508-cc40-427e-98db-f79b55c7ee3a",
   "metadata": {},
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7e03e7f-e05e-4dce-9f9b-391671f65b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/deepak/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/deepak/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/deepak/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/deepak/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/deepak/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to /home/deepak/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n",
      "[nltk_data] Downloading package treebank to /home/deepak/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/treebank.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n'punkt': This is a tokenizer that can split a text into a list of sentences or words.\\n'stopwords': This is a list of stop words in different languages.\\n'wordnet': This is a lexical database for the English language, used by the WordNetLemmatizer.\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('treebank')\n",
    "\"\"\"\n",
    "'punkt': This is a tokenizer that can split a text into a list of sentences or words.\n",
    "'stopwords': This is a list of stop words in different languages.\n",
    "'wordnet': This is a lexical database for the English language, used by the WordNetLemmatizer.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be0bed9c-a794-463e-bb23-be026e2ede13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cda81fcd-68ce-4569-a80b-e57d0c787620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sudo apt-get install python3-tk\n",
    "# Install it using terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aad15ca-2b13-4dc4-8bec-f6d6e5eef119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2261e44-d5a4-43ef-b0a9-497ae56f140c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nre: This is the regular expression library in Python. It provides support for working with regular expressions, which are used for pattern matching in strings.\\n\\npandas: This is a powerful data manipulation and analysis library in Python. It provides data structures like DataFrame for handling tabular data.\\n\\nnltk: The Natural Language Toolkit (NLTK) is a library for working with human language data (text). It provides tools for tasks such as tokenization, stemming, tagging, parsing, and more.\\n\\nstopwords: This is a module within NLTK that provides a list of common stop words in various languages. Stop words are words like \"the\", \"is\", \"in\", etc., that are often removed in text preprocessing.\\n\\nword_tokenize: This is a function in NLTK used to split a string into individual words or tokens.\\n\\nWordNetLemmatizer: This is a lemmatizer provided by NLTK that uses the WordNet lexical database to perform lemmatization. Lemmatization reduces words to their base or root form.\\n\\nCounter: This is a class in the collections module that helps count the occurrences of elements in a list or other iterable.\\n\\nWordCloud: This is a library for generating word cloud visualizations from text data.\\n\\nseaborn: This is a statistical data visualization library based on Matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\\n\\nmatplotlib.pyplot: This is a plotting library in Python. It provides an interface similar to MATLAB for creating static, interactive, and animated visualizations.\\n\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "re: This is the regular expression library in Python. It provides support for working with regular expressions, which are used for pattern matching in strings.\n",
    "\n",
    "pandas: This is a powerful data manipulation and analysis library in Python. It provides data structures like DataFrame for handling tabular data.\n",
    "\n",
    "nltk: The Natural Language Toolkit (NLTK) is a library for working with human language data (text). It provides tools for tasks such as tokenization, stemming, tagging, parsing, and more.\n",
    "\n",
    "stopwords: This is a module within NLTK that provides a list of common stop words in various languages. Stop words are words like \"the\", \"is\", \"in\", etc., that are often removed in text preprocessing.\n",
    "\n",
    "word_tokenize: This is a function in NLTK used to split a string into individual words or tokens.\n",
    "\n",
    "WordNetLemmatizer: This is a lemmatizer provided by NLTK that uses the WordNet lexical database to perform lemmatization. Lemmatization reduces words to their base or root form.\n",
    "\n",
    "Counter: This is a class in the collections module that helps count the occurrences of elements in a list or other iterable.\n",
    "\n",
    "WordCloud: This is a library for generating word cloud visualizations from text data.\n",
    "\n",
    "seaborn: This is a statistical data visualization library based on Matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\n",
    "\n",
    "matplotlib.pyplot: This is a plotting library in Python. It provides an interface similar to MATLAB for creating static, interactive, and animated visualizations.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0223c4d2-6ca8-4694-a357-4fc0e3a6da3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'NN'), ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN'), ('Arthur', 'NNP'), ('did', 'VBD'), (\"n't\", 'RB'), ('feel', 'VB'), ('very', 'RB'), ('good', 'JJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    " sentence = \"\"\"At eight o'clock on Thursday morning Arthur didn't feel very good.\"\"\"\n",
    "tokens=nltk.word_tokenize(sentence)\n",
    "tokens\n",
    "tagged=nltk.pos_tag(tokens)\n",
    "print(tagged)\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "entities\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus import treebank\n",
    "t = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
    "t.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf45541-6e10-4361-b2ed-72db1c23692b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d0f9bf-f945-4ecd-94c9-32059adab8b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7dc9d-2546-425e-b094-bc231a79d431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90445b0-e278-4629-9eb5-c31f58b76349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73df430-a5eb-48d1-971d-eea810b6a987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7100fcd0-6a0d-4b65-8519-3a4e9f3db650",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d140b0-760e-43d6-abf9-f6aa8bf519c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbffcebd-fb77-4616-ac20-3627286f36d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
